{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "# **Phrase 분석**\n",
    "\n",
    "<br></br>\n",
    "## **1 불용어 처리**\n",
    "Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i like such a wonderful snow ice cream'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = 'I like such a Wonderful Snow Ice Cream'\n",
    "texts = texts.lower()\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'like', 'such', 'a', 'wonderful', 'snow', 'ice', 'cream']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "tokens = word_tokenize(texts)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'her', 'those', 'an', 'into', 'further', 'such', 'now', 'mightn']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')[::18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['like', 'wonderful', 'snow', 'ice', 'cream']\n"
     ]
    }
   ],
   "source": [
    "tokens = [word   for word in tokens   \n",
    "                 if word not in stopwords.words('english')]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## **2 한글의 불용어 처리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'이': 'NP',\n",
       " '있': 'VA',\n",
       " '하': 'VV',\n",
       " '것': 'NNB',\n",
       " '들': 'VV',\n",
       " '그': 'MM',\n",
       " '되': 'VV',\n",
       " '수': 'NNB',\n",
       " '보': 'VX',\n",
       " '않': 'VX',\n",
       " '없': 'VA',\n",
       " '나': 'VX',\n",
       " '사람': 'NNG',\n",
       " '주': 'VV',\n",
       " '아니': 'VCN',\n",
       " '등': 'NNB',\n",
       " '같': 'VA',\n",
       " '우리': 'NP',\n",
       " '때': 'NNG',\n",
       " '년': 'NNB',\n",
       " '가': 'VV',\n",
       " '한': 'MM',\n",
       " '지': 'VX',\n",
       " '대하': 'VV',\n",
       " '오': 'VV',\n",
       " '말': 'VX',\n",
       " '일': 'NNB',\n",
       " '그렇': 'VA',\n",
       " '위하': 'VV',\n",
       " '때문': 'NNB',\n",
       " '그것': 'NP',\n",
       " '두': 'VV',\n",
       " '말하': 'VV',\n",
       " '알': 'VV',\n",
       " '그러나': 'MAJ',\n",
       " '받': 'VV',\n",
       " '못하': 'VX',\n",
       " '그런': 'MM',\n",
       " '또': 'MAG',\n",
       " '문제': 'NNG',\n",
       " '더': 'MAG',\n",
       " '사회': 'NNG',\n",
       " '많': 'VA',\n",
       " '그리고': 'MAJ',\n",
       " '좋': 'VA',\n",
       " '크': 'VA',\n",
       " '따르': 'VV',\n",
       " '중': 'NNB',\n",
       " '나오': 'VV',\n",
       " '가지': 'VV',\n",
       " '씨': 'NNB',\n",
       " '시키': 'XSV',\n",
       " '만들': 'VV',\n",
       " '지금': 'NNG',\n",
       " '생각하': 'VV',\n",
       " '그러': 'VV',\n",
       " '속': 'NNG',\n",
       " '하나': 'NR',\n",
       " '집': 'NNG',\n",
       " '살': 'VV',\n",
       " '모르': 'VV',\n",
       " '적': 'XSN',\n",
       " '월': 'NNB',\n",
       " '데': 'NNB',\n",
       " '자신': 'NNG',\n",
       " '안': 'MAG',\n",
       " '어떤': 'MM',\n",
       " '내': 'VV',\n",
       " '경우': 'NNG',\n",
       " '명': 'NNB',\n",
       " '생각': 'NNG',\n",
       " '시간': 'NNG',\n",
       " '그녀': 'NP',\n",
       " '다시': 'MAG',\n",
       " '이런': 'MM',\n",
       " '앞': 'NNG',\n",
       " '보이': 'VV',\n",
       " '번': 'NNB',\n",
       " '다른': 'MM',\n",
       " '어떻': 'VA',\n",
       " '여자': 'NNG',\n",
       " '개': 'NNB',\n",
       " '전': 'NNG',\n",
       " '사실': 'NNG',\n",
       " '이렇': 'VA',\n",
       " '점': 'NNG',\n",
       " '싶': 'VX',\n",
       " '정도': 'NNG',\n",
       " '좀': 'MAG',\n",
       " '원': 'NNB',\n",
       " '잘': 'MAG',\n",
       " '통하': 'VV',\n",
       " '소리': 'NNG',\n",
       " '놓': 'VX'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('./data/한국어불용어100.txt', 'r')\n",
    "s = f.read(); f.close()\n",
    "\n",
    "stop_words = [ txt.split('\\t')[:2]  for txt in s.split('\\n') ]\n",
    "stopword   = {}\n",
    "for txt in stop_words:\n",
    "    try:    stopword[txt[0]] = txt[1]\n",
    "    except: pass\n",
    "stopword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## **3 Token 객체를 활용한 통계적 분석**\n",
    "1. 레벤슈타인의 편집거리\n",
    "1. Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 12, 12)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"자연 언어에 대한 연구는 오래전부터 이어져 오고 있음에도 2018년까지도 사람처럼 이해하지는 못한다.\".split()\n",
    "text2 = \"자연 언어에 대한 연구는 오래전부터 이어져 들어서도 아직 컴퓨터가 사람처럼 이해하지는 못한다.\".split()\n",
    "text3 = \"자연 아직 컴퓨터가 언어에 들어서도 못한다 이어져 사람처럼 이해하지는 대한 연구는 오래전부터.\".split()\n",
    "len(text1), len(text2), len(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.metrics import edit_distance\n",
    "edit_distance('파이썬 알고리즘', '파파미 알탕')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생략된 단어가 다를 때 : 3 \n",
      "어휘 순서를 바꿨을 때 : 10\n"
     ]
    }
   ],
   "source": [
    "print('생략된 단어가 다를 때 : {} \\n어휘 순서를 바꿨을 때 : {}'.format(\n",
    "    edit_distance(text1, text2), \n",
    "    edit_distance(text2, text3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 02 accuracy 정확도 측정\n",
    "from nltk.metrics import accuracy\n",
    "accuracy('파이썬', '파이프')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생략된 단어가 다를 때 0.75 \n",
      "어휘 순서를 바꿨을 때 0.08333\n"
     ]
    }
   ],
   "source": [
    "print('생략된 단어가 다를 때 {:.4} \\n어휘 순서를 바꿨을 때 {:.4}'.format(\n",
    "    accuracy(text1, text2), \n",
    "    accuracy(text2, text3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## **4 Token 고유 객체를 활용한 통계적 검증방법**\n",
    "1. precision = Correct / (Correct + Incorrect + Missing)\n",
    "1. recall = Correct / (Correct + Incorrect + Spurious<가짜>)\n",
    "1. f_measure = (2 X Precision X Recall) / (Precision + Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 12, 12)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = set(text1)\n",
    "text2 = set(text2)\n",
    "text3 = set(text3)\n",
    "len(text1), len(text2), len(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.metrics import precision\n",
    "precision({'파이썬'}, {'파르썬'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생략된 단어가 다를 때 0.75 \n",
      "어휘 순서를 바꿨을 때 0.8333\n"
     ]
    }
   ],
   "source": [
    "print('생략된 단어가 다를 때 {:.4} \\n어휘 순서를 바꿨을 때 {:.4}'.format(\n",
    "    precision(set(text1), set(text2)), \n",
    "    precision(set(text2), set(text3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생략된 단어가 다를 때 0.75 \n",
      "어휘 순서를 바꿨을 때 0.8333\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import recall\n",
    "print('생략된 단어가 다를 때 {:.4} \\n어휘 순서를 바꿨을 때 {:.4}'.format(\n",
    "    recall(text1, text2), \n",
    "    recall(text2, text3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생략된 단어가 다를 때 0.75 \n",
      "어휘 순서를 바꿨을 때 0.8333\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import f_measure\n",
    "print('생략된 단어가 다를 때 {:.4} \\n어휘 순서를 바꿨을 때 {:.4}'.format(\n",
    "    f_measure(text1, text2), \n",
    "    f_measure(text2, text3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "# **3 N-gram 의 활용**\n",
    "<br></br>\n",
    "## **1 N-gram 생성하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 독일 퀘르버 재단 연설문 : 베를린 선언\n",
    "f     = open('./data/베를린선언.txt', 'r')\n",
    "texts_org = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_token = word_tokenize(texts_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('존경하는', '독일', '국민'),\n",
       " ('독일', '국민', '여러분'),\n",
       " ('국민', '여러분', ','),\n",
       " ('여러분', ',', '고국에'),\n",
       " (',', '고국에', '계신')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "texts_sample = [txt for txt in ngrams(word_token, 3)]\n",
    "texts_sample[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## **2-1 Point wise Mutual Information**\n",
    "PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['존경하는', '독일', '국민', '여러분', '고국에', '계신', '국민', '여러분', '하울젠', '쾨르버재단']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "re_capt = RegexpTokenizer('[가-힣]\\w+')\n",
    "raw_texts = re_capt.tokenize(texts_org)\n",
    "raw_texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'존경하는 독일 국민 여러분 고국에 계신 국민 여러분 하울젠 쾨르버재단 이사님과 모드로 동독 총리님을 비롯한 내외 귀빈 여러분 먼저 냉전과 분단을 넘어 통일을 이루고 힘으로 유럽통합과 국제평화를 선도하고 있는 독일과 독일 국민에게 무한한 경의를 표합니다 오늘 자리를 마련해 주신 독일 정부와 쾨르버 재단에도 감사드립니다 아울러 얼마 별세하신 헬무트 총리의 가족'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = ''\n",
    "for txt in raw_texts:\n",
    "    texts += txt + \" \"\n",
    "texts[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markbaum/Python/nltk/lib/python3.6/site-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.19 s, sys: 151 ms, total: 8.34 s\n",
      "Wall time: 3.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 베를린 선언문에 Tag 속성 추가하기\n",
    "from konlpy.tag import Twitter\n",
    "twitter = Twitter()\n",
    "tagged_words = twitter.pos(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## **2-2 Bi-Gram 을 대상으로 한 PMI**\n",
    "최상위 우도값 10개를 추출한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.collocations.BigramCollocationFinder at 0x7f2c65bb6a90>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import collocations\n",
    "\n",
    "finder = collocations.BigramCollocationFinder.from_words(tagged_words)\n",
    "finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('가능하며', 'Adjective'), ('불가', 'Noun')),\n",
       " (('가리지', 'Verb'), ('않습니다', 'Verb')),\n",
       " (('가스', 'Noun'), ('관', 'Noun')),\n",
       " (('가운데', 'Noun'), ('현재', 'Noun')),\n",
       " (('감사', 'Noun'), ('드립니다', 'Verb')),\n",
       " (('같은', 'Adjective'), ('공감', 'Noun')),\n",
       " (('건너지', 'Verb'), ('않기를', 'Verb')),\n",
       " (('검증', 'Noun'), ('가능하며', 'Adjective')),\n",
       " (('견', 'Noun'), ('지하', 'Noun')),\n",
       " (('겹', 'Noun'), ('치는', 'Verb'))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measures = collocations.BigramAssocMeasures()\n",
    "finder.nbest(measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## **2-3 Tri-Gram 을 대상으로한 PMI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.collocations.TrigramCollocationFinder at 0x7f2c65bb6c50>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder = collocations.TrigramCollocationFinder.from_words(tagged_words)\n",
    "finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('가능하며', 'Adjective'), ('불가', 'Noun'), ('역적', 'Noun')),\n",
       " (('가스', 'Noun'), ('관', 'Noun'), ('연결', 'Noun')),\n",
       " (('가운데', 'Noun'), ('현재', 'Noun'), ('생존', 'Noun')),\n",
       " (('같은', 'Adjective'), ('공감', 'Noun'), ('대', 'Suffix')),\n",
       " (('건너지', 'Verb'), ('않기를', 'Verb'), ('바랍니다', 'Verb')),\n",
       " (('검증', 'Noun'), ('가능하며', 'Adjective'), ('불가', 'Noun')),\n",
       " (('견', 'Noun'), ('지하', 'Noun'), ('면서', 'Noun')),\n",
       " (('과도', 'Josa'), ('같은', 'Adjective'), ('공감', 'Noun')),\n",
       " (('닦아', 'Verb'), ('주어', 'Noun'), ('야', 'Josa')),\n",
       " (('마다', 'Josa'), ('흔들리거나', 'Verb'), ('깨져서도', 'Verb'))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measures = collocations.TrigramAssocMeasures()\n",
    "finder.nbest(measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "# **4 TF-IDF**\n",
    "Term Frequency-Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## **1 영문 데이터 전처리**\n",
    "트럼프 취임사 연설문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chief', 'Justice', 'Roberts', ',', 'President']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('./data/trump.txt', 'r')\n",
    "texts_org = f.read()\n",
    "f.close()\n",
    "\n",
    "from nltk import word_tokenize\n",
    "texts = word_tokenize(texts_org)\n",
    "texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopword_eng = stopwords.words('english')\n",
    "\n",
    "import string\n",
    "punct = string.punctuation\n",
    "punct = [punct[i] for i in range(len(punct))]\n",
    "punct = punct + stopword_eng + ['\\n'] \n",
    "len(punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [txt.lower()    for txt in texts   if txt.lower() not in punct]\n",
    "document = ''\n",
    "for txt in texts:\n",
    "    document += txt + ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## **2 tf idf**\n",
    "연설문내 단어들의 빈도를 재조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.83 ms, sys: 41 µs, total: 4.87 ms\n",
      "Wall time: 4.17 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vec   = TfidfVectorizer()\n",
    "transformed = tfidf_vec.fit_transform(raw_documents = [document])\n",
    "index_value = {i[1]:i[0] for i in tfidf_vec.vocabulary_.items()}\n",
    "\n",
    "transformed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "fully_indexed = []\n",
    "\n",
    "import numpy as np\n",
    "transformed = np.array(transformed.todense())\n",
    "\n",
    "for row in transformed:\n",
    "    fully_indexed.append({index_value[column]:value for (column,value) in enumerate(row)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "america     0.423619\n",
       "american    0.232990\n",
       "people      0.211809\n",
       "country     0.190628\n",
       "nation      0.190628\n",
       "one         0.169447\n",
       "every       0.148266\n",
       "never       0.127086\n",
       "world       0.127086\n",
       "back        0.127086\n",
       "new         0.127086\n",
       "great       0.127086\n",
       "make        0.105905\n",
       "god         0.105905\n",
       "today       0.105905\n",
       "dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tfidf = pd.Series(fully_indexed[0]).sort_values(ascending=False)\n",
    "tfidf[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12708556268223822"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf['great']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
